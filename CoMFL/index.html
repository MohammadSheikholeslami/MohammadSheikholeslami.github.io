<!DOCTYPE html>
<html>
<head>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <style>
  img {
    display: block;
    margin-left: auto;
    margin-right: auto;
  }
  </style>
  
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Multimodal FL for HAR</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Towards Collaborative Multi-modal Federated Learning for
              Human Activity Recognition in Smart Workplace Environments</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">S. Mohammad Sheikholeslami</a><sup>1</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Pai Chet Ng</a><sup>1</sup>,</span>
                   <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Huan Liu</a><sup>2</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="Forth AUTHOR PERSONAL LINK" target="_blank">Yuanhao Yu</a><sup>2</sup>,</span>
                  </span>
                  <span class="author-block">
                    <a href="Fivth AUTHOR PERSONAL LINK" target="_blank">Konstantinos N. Plataniotis</a><sup>1</sup>,</span>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>Department of Electrical and Computer Engineering, University of Toronto </span><br>
                    <span class="author-block"><sup>2</sup>Huawei Noah’s Ark Lab, Canada</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="static/pdfs/ICASSP.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link 
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/FL-HAR/CoMFL/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>  -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        
        <source src="static/images/CoMFL_Super.png"
        type="image/png">
      </video>
      <h2 class="subtitle has-text-centered">
        Our proposed CoMFL consists of Intra-zone for local collaborative training between 
        local devices enabling efficient
        processing of multiple modalities from multiple devices, 
        and Inter-zone for facilitating global federated learning,
      </h2>
    </div>
  </div>
</section>
End teaser video -->>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This paper aims to improve human activity recognition
            (HAR) with multimodal data across multiple consumer devices
            in a smart workplace environment. By leveraging the
            sensor-rich capabilities of smartphones, smartwatches, and
            smart speakers, we propose Collaborative Multimodal Federated
            Learning (CoMFL) algorithm to facilitate efficient
            feature encoding on lightweight local models implemented
            on consumer devices while fusing these encoded features
            for training a super model on a personalized local server,
            all within the private zone of a user. Federated learning aggregates
            model updates across users without compromising
            privacy, resulting in a generalized global super model. Additionally,
            we address the challenge of missing modality by
            incorporating a feature reconstruction network. This network
            attempts to reconstruct missing modalities prior to feature
            fusion, improving performance when dealing with missing
            features. Our proposed CoMFL achieves significant performance
            gains with multimodal HAR systems.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        
        <img src="static/images/CoMFL_Super.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Our proposed CoMFL consists of Intra-zone for local collaborative training between 
          local devices enabling efficient
          processing of multiple modalities from multiple devices, and Inter-zone for facilitating 
          global federated learning,
        </h2>
      </div>
      <div class="item">
        
        <img src="static/images/CoMFL_PrivateZone.png" alt="MY ALT TEXT" width="700" height="500" class="center"/>
        <h2 class="subtitle has-text-centered">
          A private zone.
        </h2>
      </div>
      <div class="item">
        
        <img src="static/images/reconstruction.png" alt="MY ALT TEXT" width="700" height="500" class="center"/>
        <h2 class="subtitle has-text-centered">
          modality
          reconstruction network to reconstruct the feature (indicated by the red dashed line) lost due
           to the communication failure.
       </h2>
     </div>
  </div>
</div>
</div>
</section>
 -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introudction and Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Human activity recognition (HAR) has witnessed rapid advancement
            with the proliferation of consumer devices embedded
            with heterogeneous sensors that capture multimodal
            data providing richer contextual information for HAR in realworld
            scenarios. However, data over communication networks is distributed 
            over different devices and users are reluctant to share their private data with a second party.
            In this regard, mplementing a multimodal HAR system across
            heterogeneous devices within smart workplace environments
            presents several challenges: 1) effective fusion of multimodal
            data to build a unified model for all modalities is challenging
            particularly due to cross-modal data heterogeneity and the
            asynchronous nature of data sampling across devices, and 2)
            inaccessibility to diverse data from different users degrades
            the generalization performance of the HAR model.
            <img src="static/images/Motivation2.png" alt="MY ALT TEXT"/>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Proposed CoMFL framework</h2>
        <div class="content has-text-justified">
          <p> 
          Our proposed CoMFL consists of Intra-zone for local collaborative training between 
          local devices enabling efficient
          processing of multiple modalities from multiple devices, and Inter-zone for facilitating 
          global federated learning,
            <img src="static/images/CoMFL_Super.png" alt="MY ALT TEXT"/>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>
        <div class="content has-text-justified">
          <div class="subsection">
            <h3 class="subtitle is-4">Dataset Setup</h3>
            <p>
              We examine a scenario involving N = 28 individuals engaged in
               9 distinct activities. This data was collected through our data collection campaign,
                with the informed consent of all participants. For each device, we use 2 
                CNN layers with 64 and 32 and filters of size 3 × 3 and max-pooling 2 × 2. The super
                model is composed of two fully-connected layers with 64 and 32 neurons, respectively.
              <img src="static/images/Dataset_setup.png" alt="MY ALT TEXT" width="500" height="400"/>
            </p>
          </div>
          <div class="subsection">
            <h3 class="subtitle is-4">Simulation Results</h3>
            <p>
              Our simulation results shows a) phone modality reaches the best 
              performance among unimodal HAR models, b) integrating multi-modality enhances accuracy of 
              the model, c) the distributed learing approach results in a less accurate model compared to the 
              centralized setting with the benefit of retaining data privacy.
            </p>
          </div>
          <p>
            <img src="static/images/TestAccuracy.png" alt="MY ALT TEXT" width="400" height="300"/>

            We consider a scenario where one of the modalities is missing due to 
            hardware or communication failure. WE compare (i) baseline where we replace the
            missing modality with zeros, and (ii) the missing data is reconstructed
            using the proposed method. As can be seen from
            the table, phone modality has the highest effect on the model
            performance. However, in practice, we observed that the
            smartwatch usually fails to provide its captured data
            <img src="static/images/Reconstruction_table.png" alt="MY ALT TEXT" width="1000" height="900"/>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video 
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
           
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
 -->


<!-- Video carousel 
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>
End video carousel -->






<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/COMFL_Poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{Sheikholeslami_CoMFL2024,
        title={Towards Collaborative Multi-modal Federated Learning for Human Activity Recognition in Smart 
        Workplace Environments},
        author={Sheikholeslami, Seyed Mohammad and Ng, Pai Chet and Liu, Huan and Yu, Yuanhao and Plataniotis, Konstantinos N.},
        journal={IEEE International Conference on Acoustics, Speech and Signal Processing Workshop (ICASSPW)},
        year={2024}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
